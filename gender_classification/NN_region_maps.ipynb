{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_region_maps.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HBxwQ-1sOpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import argparse as ap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-pDgJinu0ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/My Drive/Computer_Vision_final/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMt6_OUrV4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYS_1km2XnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the stuff for testing \n",
        "csv_path = data_dir + \"face_labels.csv\"\n",
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Garxkvc04Hs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()\n",
        "col_file = list(df.file)\n",
        "col_file = col_file[:-2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAcBHc7SVOF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-cznjGr2rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RUN THIS TO HAVE REN AS TEST\n",
        "images = list()\n",
        "labels = list()\n",
        "#primary_path = data_dir + \"region_maps/\"\n",
        "#sub_dir_list = os.listdir(primary_path)\n",
        "for i in range(len(col_file)):\n",
        "  label = int(df['gender'][i])\n",
        "  holder = col_file[i].split(\".\")\n",
        "  name = holder[0] + \".jpg\"\n",
        "  path = data_dir + \"region_maps/\" + name\n",
        "  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
        "  image_data = np.array(img)\n",
        "  images.append(image_data)\n",
        "  labels.append(label)\n",
        "x = np.array(images)\n",
        "# test_features = np.array(images)\n",
        "# labels = np.array(labels)\n",
        "y = np.array(keras.utils.to_categorical(np.array(labels), num_classes=2))\n",
        "# test_labels = np.array(keras.utils.to_categorical(np.array(labels), num_classes=2)) \n",
        "train_features , test_features ,train_labels, test_labels = train_test_split( x , y , test_size=0.4 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5uC1ZZAvjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RUN THIS TO DO REN ON REN \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQSRTHmMQFG",
        "colab_type": "text"
      },
      "source": [
        "We have just loaded the contour maps in. Now what we want to do is to create a NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_u11HsRJKh",
        "colab_type": "text"
      },
      "source": [
        "FIRST WE WILL LOAD IN OUR TRAIN DATA>> SORRY IT IS BACKWARD!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iNCJLT9RIEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the stuff for testing \n",
        "csv_path_train = data_dir + \"face_train.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "col_file_train = list(df_train.file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOENCKKOXYqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_file_train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jh7ZzCpPz0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_train = list()\n",
        "labels_train = list()\n",
        "# primary_path = data_dir + \"corners/train/\"\n",
        "for i in range(len(col_file_train)):\n",
        "  label = int(df_train['gender'][i])\n",
        "  holder = col_file_train[i].split(\".\")\n",
        "  name = holder[0] + \".jpg\"\n",
        "  path = data_dir + \"region_maps_train/\" + name\n",
        "  print(path)\n",
        "  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
        "  image_data = np.array(img)\n",
        "  \n",
        "  if image_data.shape == (0,0):\n",
        "    continue \n",
        "  \n",
        "  images_train.append(image_data)\n",
        "  labels_train.append(label)\n",
        "\n",
        "train_features = np.array(images_train)\n",
        "labels_train = np.array(labels_train)\n",
        "#y = np.array(keras.utils.to_categorical(np.array(labels_train), num_classes=2))\n",
        "train_labels = np.array(keras.utils.to_categorical(np.array(labels_train), num_classes=2))\n",
        "#train_features , test_features ,train_labels, test_labels = train_test_split( x_features , y , test_size=0.4 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csdyV4BTFyiB",
        "colab_type": "code",
        "outputId": "e11cfd14-84ae-4a23-88e4-c7bcc2d3ddbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 128, 128)\n",
            "(68, 128, 128)\n",
            "(101, 2)\n",
            "(68, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOdHNKosvFJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtN3kOxxMZ4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import models , optimizers , losses ,activations\n",
        "from tensorflow.python.keras.layers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1cte0DMrFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(object):\n",
        "    def __init__( self , number_of_classes, dimensions):\n",
        "        dropout_rate = 0.5\n",
        "        self.dimensions = dimensions\n",
        "        input_shape = ( dimensions**2 , )\n",
        "        convolution_shape = ( dimensions , dimensions , 1 )\n",
        "        kernel_size = (3 , 3)\n",
        "        pool_size = (2 , 2)\n",
        "        strides = 1\n",
        "        activation_func = activations.relu\n",
        "        self.layers = [\n",
        "            Reshape( input_shape=input_shape , target_shape=convolution_shape),\n",
        "            Conv2D( 32, kernel_size=( 4 , 4 ) , strides=strides , activation=activation_func),\n",
        "            MaxPooling2D(pool_size=pool_size, strides=strides ),\n",
        "            Conv2D( 64, kernel_size=( 3 , 3 ) , strides=strides , activation=activation_func),\n",
        "            MaxPooling2D(pool_size=pool_size , strides=strides),\n",
        "            Flatten(),\n",
        "            Dense( 100, activation=activation_func) ,\n",
        "            Dropout(dropout_rate),\n",
        "            Dense( number_of_classes, activation=tf.nn.softmax )\n",
        "        ]\n",
        "        self.model = tf.keras.Sequential(self.layers)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(lr=.001),\n",
        "            loss=losses.categorical_crossentropy ,\n",
        "            metrics=['accuracy'] ,\n",
        "        )\n",
        "    def fit(self, X, Y, hyperparameters):\n",
        "        #if you want to not have that epoch printing you can just say verbose  = 0 in the .fit call \n",
        "        self.model.fit(X, Y, batch_size=hyperparameters['batch_size'], epochs=hyperparameters['epochs'], shuffle = True)  \n",
        "\n",
        "    def evaluate(self , test_X , test_Y) :\n",
        "        return self.model.evaluate(test_X, test_Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        return predictions\n",
        "\n",
        "    def save_model(self , file_path ):\n",
        "        self.model.save(file_path )\n",
        "\n",
        "    def load_model(self , file_path ):\n",
        "        self.model = models.load_model(file_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6EW2nKPd8z",
        "colab_type": "text"
      },
      "source": [
        "Now to try and run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcDFOc8kPdAK",
        "colab_type": "code",
        "outputId": "edad542c-faaa-40cf-ae0e-20c47e05d69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "data_dimension = 128\n",
        "train_features =  train_features.reshape( (  train_features.shape[0] , data_dimension**2  ) ).astype( np.float32 )\n",
        "test_features= test_features.reshape( ( test_features.shape[0] , data_dimension**2 ) ).astype( np.float32 )\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "classifier = Classifier( number_of_classes=2, dimensions = 128 )\n",
        "parameters = {\n",
        "    'batch_size' : 50 ,\n",
        "    'epochs' : 16,\n",
        "}\n",
        "classifier.fit(train_features , train_labels  , hyperparameters=parameters )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 16384)\n",
            "(68, 16384)\n",
            "(101, 2)\n",
            "(68, 2)\n",
            "Train on 101 samples\n",
            "Epoch 1/24\n",
            "101/101 [==============================] - 0s 4ms/sample - loss: 0.6916 - acc: 0.5743\n",
            "Epoch 2/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 1.0298 - acc: 0.5941\n",
            "Epoch 3/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.7617 - acc: 0.5644\n",
            "Epoch 4/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6789 - acc: 0.5644\n",
            "Epoch 5/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6978 - acc: 0.4455\n",
            "Epoch 6/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6738 - acc: 0.5545\n",
            "Epoch 7/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6775 - acc: 0.5050\n",
            "Epoch 8/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6792 - acc: 0.4356\n",
            "Epoch 9/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6775 - acc: 0.5446\n",
            "Epoch 10/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.7210 - acc: 0.5842\n",
            "Epoch 11/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6693 - acc: 0.5941\n",
            "Epoch 12/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.7097 - acc: 0.5149\n",
            "Epoch 13/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.8479 - acc: 0.5149\n",
            "Epoch 14/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.7479 - acc: 0.5347\n",
            "Epoch 15/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 1.0097 - acc: 0.5842\n",
            "Epoch 16/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 1.0505 - acc: 0.5545\n",
            "Epoch 17/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6989 - acc: 0.5644\n",
            "Epoch 18/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.5446\n",
            "Epoch 19/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6648 - acc: 0.6040\n",
            "Epoch 20/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6712 - acc: 0.5842\n",
            "Epoch 21/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6676 - acc: 0.5842\n",
            "Epoch 22/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6635 - acc: 0.5842\n",
            "Epoch 23/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6709 - acc: 0.5842\n",
            "Epoch 24/24\n",
            "101/101 [==============================] - 0s 2ms/sample - loss: 0.6714 - acc: 0.5842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KJSIoPkWFfi",
        "colab_type": "code",
        "outputId": "226a930a-61f5-41c1-f415-6a91eef21629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#classifier.save_model( 'models/model.h5')\n",
        "preds = classifier.predict(test_features)\n",
        "# loss , accuracy, FP, AUC, TP = classifier.evaluate( test_features , test_labels )\n",
        "# print( \"Loss of {}\".format( loss ) , \"Accuracy of {} %\".format( accuracy * 100 ), \"False Positive {}\".format( FP ), \" TruePositive {}\".format( TP ), \"Area Under Curve{}\".format( AUC ) )\n",
        "loss , accuracy = classifier.evaluate( test_features , test_labels )\n",
        "print( \"Loss of {}\".format( loss ) , \"Accuracy of {} %\".format( accuracy * 100 ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68/68 [==============================] - 0s 2ms/sample - loss: 0.7055 - acc: 0.5000\n",
            "Loss of 0.7055284731528338 Accuracy of 50.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}