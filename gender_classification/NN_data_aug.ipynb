{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_data_aug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HBxwQ-1sOpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import argparse as ap\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-pDgJinu0ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/My Drive/Computer_Vision_final/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMt6_OUrV4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "dc2a1218-fe81-43f9-a922-3a23ae6d541f"
      },
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKYS_1km2XnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the stuff for testing \n",
        "csv_path = data_dir + \"face_labels.csv\"\n",
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Garxkvc04Hs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()\n",
        "col_file = list(df.file)\n",
        "col_file = col_file[:-2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-cznjGr2rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = list()\n",
        "labels = list()\n",
        "primary_path = data_dir + \"corners/\"\n",
        "#sub_dir_list = os.listdir(primary_path)\n",
        "for i in range(len(col_file)):\n",
        "  label = int(df['gender'][i])\n",
        "  holder = col_file[i].split(\".\")\n",
        "  name = holder[0] + \"_corners.jpg\"\n",
        "  path = data_dir + \"corners/\" + name\n",
        "  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
        "  image_data = np.array(img)\n",
        "  images.append(image_data)\n",
        "  labels.append(label)\n",
        "# x = np.array(images)\n",
        "test_features = np.array(images)\n",
        "labels = np.array(labels)\n",
        "# y = np.array(keras.utils.to_categorical(np.array(labels), num_classes=2))\n",
        "test_labels = np.array(keras.utils.to_categorical(np.array(labels), num_classes=2))\n",
        "# train_features , test_features ,train_labels, test_labels = train_test_split( x , y , test_size=0.4 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQSRTHmMQFG",
        "colab_type": "text"
      },
      "source": [
        "We have just loaded the contour maps in. Now what we want to do is to create a NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_u11HsRJKh",
        "colab_type": "text"
      },
      "source": [
        "FIRST WE WILL LOAD IN OUR TRAIN DATA>> SORRY IT IS BACKWARD!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iNCJLT9RIEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the stuff for testing \n",
        "csv_path_train = data_dir + \"face_train.csv\"\n",
        "# csv_path_train = data_dir + \"face_labels.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "col_file_train = list(df_train.file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOENCKKOXYqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_file_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jh7ZzCpPz0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_train = list()\n",
        "labels_train = list()\n",
        "primary_path = data_dir + \"corners/train/\"\n",
        "for i in range(len(col_file_train)):\n",
        "  label = int(df_train['gender'][i])\n",
        "  holder = col_file_train[i].split(\".\")\n",
        "  name = holder[0] + \"_corners.jpg\"\n",
        "  path = data_dir + \"corners/train/\" + name\n",
        "  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n",
        "  image_data = np.array(img)\n",
        "  \n",
        "  if image_data.shape == (0,0):\n",
        "    continue \n",
        "  \n",
        "  images_train.append(image_data)\n",
        "  labels_train.append(label)\n",
        "\n",
        "train_features = np.array(images_train)\n",
        "labels_train = np.array(labels_train)\n",
        "#y = np.array(keras.utils.to_categorical(np.array(labels_train), num_classes=2))\n",
        "train_labels = np.array(keras.utils.to_categorical(np.array(labels_train), num_classes=2))\n",
        "#train_features , test_features ,train_labels, test_labels = train_test_split( x_features , y , test_size=0.4 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csdyV4BTFyiB",
        "colab_type": "code",
        "outputId": "684c9193-5694-447d-d895-8b5610988c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1615, 128, 128)\n",
            "(169, 128, 128)\n",
            "(1615, 2)\n",
            "(169, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOdHNKosvFJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtN3kOxxMZ4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import models , optimizers , losses ,activations\n",
        "from tensorflow.python.keras.layers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1cte0DMrFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(object):\n",
        "    def __init__( self , number_of_classes, dimensions):\n",
        "        dropout_rate = 0.5\n",
        "        self.dimensions = dimensions\n",
        "        # input_shape = ( dimensions**2 , )\n",
        "        input_shape = (128,128,1)\n",
        "        convolution_shape = ( dimensions , dimensions , 1 )\n",
        "        kernel_size = (3 , 3)\n",
        "        pool_size = (2 , 2)\n",
        "        strides = 1\n",
        "        activation_func = activations.relu\n",
        "        self.layers = [\n",
        "            Reshape( input_shape=input_shape , target_shape=convolution_shape),\n",
        "            Conv2D( 32, kernel_size=( 4 , 4 ) , strides=strides , activation=activation_func),\n",
        "            MaxPooling2D(pool_size=pool_size, strides=strides ),\n",
        "            Conv2D( 64, kernel_size=( 3 , 3 ) , strides=strides , activation=activation_func),\n",
        "            MaxPooling2D(pool_size=pool_size , strides=strides),\n",
        "            Flatten(input_shape=(128, 128, 1)),\n",
        "            Dense( 100, activation=activation_func) ,\n",
        "            Dropout(dropout_rate),\n",
        "            Dense( number_of_classes, activation=tf.nn.softmax )\n",
        "        ]\n",
        "        self.model = tf.keras.Sequential(self.layers)\n",
        "        # self.model.add(Flatten(input_shape=(128,128,1)))\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(lr=.001),\n",
        "            loss=losses.categorical_crossentropy ,\n",
        "            metrics=['accuracy'] ,\n",
        "        )\n",
        "    def fit(self, X, Y, hyperparameters):\n",
        "        #if you want to not have that epoch printing you can just say verbose  = 0 in the .fit call \n",
        "        aug = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            zoom_range=0.30,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            # shear_range=0.15,\n",
        "            fill_mode=\"nearest\",\n",
        "            horizontal_flip=True)\n",
        "        # self.model.fit(X, Y, batch_size=hyperparameters['batch_size'], epochs=hyperparameters['epochs'], shuffle = True)  \n",
        "        eps = hyperparameters['epochs']\n",
        "        bs = hyperparameters['batch_size']\n",
        "        self.model.fit_generator(aug.flow(X, Y, batch_size=bs, shuffle = True), steps_per_epoch=math.ceil(len(X)/bs), epochs=eps)\n",
        "\n",
        "    def evaluate(self , test_X , test_Y) :\n",
        "        return self.model.evaluate(test_X, test_Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        return predictions\n",
        "\n",
        "    def save_model(self , file_path ):\n",
        "        self.model.save(file_path )\n",
        "\n",
        "    def load_model(self , file_path ):\n",
        "        self.model = models.load_model(file_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZMt6tjMg6Qp",
        "colab_type": "code",
        "outputId": "f977e74a-62bd-48ee-9fda-47d0b907d878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data_dimension = 128\n",
        "# train_features =  train_features.reshape( (  train_features.shape[0] , data_dimension**2  ) ).astype( np.float32 )\n",
        "# test_features= test_features.reshape( ( test_features.shape[0] , data_dimension**2 ) ).astype( np.float32 )\n",
        "r,c,d = train_features.shape\n",
        "train_features = train_features.reshape((r,c,d,1))\n",
        "r,c,d = test_features.shape\n",
        "test_features = test_features.reshape((r,c,d,1))\n",
        "\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1615, 128, 128, 1)\n",
            "(169, 128, 128, 1)\n",
            "(1615, 2)\n",
            "(169, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6EW2nKPd8z",
        "colab_type": "text"
      },
      "source": [
        "Now to try and run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcDFOc8kPdAK",
        "colab_type": "code",
        "outputId": "f4b004d9-cb82-4abc-a644-ad9ba5e6bc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "classifier = Classifier( number_of_classes=2, dimensions = 128 )\n",
        "parameters = {\n",
        "    'batch_size' : 20 ,\n",
        "    'epochs' : 45,\n",
        "}\n",
        "\n",
        "classifier.fit(train_features , train_labels  , hyperparameters=parameters )\n",
        "\n",
        "\n",
        "# classifier.fit_generator(aug.flow(train_features , train_labels, batch_size=parameters.batch_size), steps_per_epoch=(len(train_features) / parameters.batch_size), epochs=parameters.epochs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "81/81 [==============================] - 20s 247ms/step - loss: 143.7331 - acc: 0.5511\n",
            "Epoch 2/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6947 - acc: 0.5653\n",
            "Epoch 3/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6520 - acc: 0.6260\n",
            "Epoch 4/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6524 - acc: 0.6322\n",
            "Epoch 5/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6563 - acc: 0.6241\n",
            "Epoch 6/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6378 - acc: 0.6440\n",
            "Epoch 7/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6274 - acc: 0.6594\n",
            "Epoch 8/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6229 - acc: 0.6619\n",
            "Epoch 9/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6277 - acc: 0.6514\n",
            "Epoch 10/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6184 - acc: 0.6774\n",
            "Epoch 11/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6091 - acc: 0.6867\n",
            "Epoch 12/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5960 - acc: 0.6854\n",
            "Epoch 13/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.6021 - acc: 0.6879\n",
            "Epoch 14/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6138 - acc: 0.6780\n",
            "Epoch 15/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6004 - acc: 0.6644\n",
            "Epoch 16/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5925 - acc: 0.6799\n",
            "Epoch 17/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.6094 - acc: 0.6867\n",
            "Epoch 18/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5950 - acc: 0.6898\n",
            "Epoch 19/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5895 - acc: 0.6947\n",
            "Epoch 20/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5780 - acc: 0.6954\n",
            "Epoch 21/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5924 - acc: 0.6960\n",
            "Epoch 22/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5873 - acc: 0.6848\n",
            "Epoch 23/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5786 - acc: 0.6916\n",
            "Epoch 24/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5911 - acc: 0.6805\n",
            "Epoch 25/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5972 - acc: 0.6836\n",
            "Epoch 26/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5854 - acc: 0.6941\n",
            "Epoch 27/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5687 - acc: 0.6923\n",
            "Epoch 28/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5990 - acc: 0.6780\n",
            "Epoch 29/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5682 - acc: 0.6904\n",
            "Epoch 30/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5697 - acc: 0.7077\n",
            "Epoch 31/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5842 - acc: 0.7040\n",
            "Epoch 32/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5938 - acc: 0.6824\n",
            "Epoch 33/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5753 - acc: 0.7003\n",
            "Epoch 34/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5784 - acc: 0.7096\n",
            "Epoch 35/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5708 - acc: 0.6947\n",
            "Epoch 36/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5605 - acc: 0.7158\n",
            "Epoch 37/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5993 - acc: 0.6941\n",
            "Epoch 38/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5757 - acc: 0.7090\n",
            "Epoch 39/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5759 - acc: 0.6892\n",
            "Epoch 40/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5720 - acc: 0.7009\n",
            "Epoch 41/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5727 - acc: 0.7046\n",
            "Epoch 42/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5645 - acc: 0.7152\n",
            "Epoch 43/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5791 - acc: 0.7053\n",
            "Epoch 44/45\n",
            "81/81 [==============================] - 17s 212ms/step - loss: 0.5800 - acc: 0.6966\n",
            "Epoch 45/45\n",
            "81/81 [==============================] - 17s 213ms/step - loss: 0.5748 - acc: 0.7133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KJSIoPkWFfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aa47731c-9ff0-43cb-dfec-7a96264a02a7"
      },
      "source": [
        "#classifier.save_model( 'models/model.h5')\n",
        "preds = classifier.predict(test_features)\n",
        "# loss , accuracy, FP, AUC, TP = classifier.evaluate( test_features , test_labels )\n",
        "loss , accuracy = classifier.evaluate( test_features , test_labels )\n",
        "# print( \"Loss of {}\".format( loss ) , \"Accuracy of {} %\".format( accuracy * 100 ), \"False Positive {}\".format( FP ), \" TruePositive {}\".format( TP ), \"Area Under Curve{}\".format( AUC ) )\n",
        "print( \"Loss of {}\".format( loss ) , \"Accuracy of {} %\".format( accuracy * 100 ))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 1ms/sample - loss: 0.5829 - acc: 0.6686\n",
            "Loss of 0.5829198173517306 Accuracy of 66.8639063835144 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOyDomomJqI5",
        "colab_type": "text"
      },
      "source": [
        "Training on BAM, testing on Renaissance\n",
        "* 20 epochs. Accuracy on train:0.7133, test:60.947\n",
        "* 40 epochs. Acc train:0.7474 test: 65.089 %\n",
        "* 80 epochs. acc train: 0.7288, test:63.905%\n",
        "* more data aug: \n",
        "  * acc train:0.7579 test: 66.2721\n",
        "  * 100 epochs train: 0.7424 test:65.08875\n",
        "  * 45 epochs, train: 0.7245 test: 68.63905191421509 %\n",
        "\n",
        "Training and Testing on Renaissance\n",
        "* 25 epochs, train: 25, test: 52.941179275512695 % "
      ]
    }
  ]
}