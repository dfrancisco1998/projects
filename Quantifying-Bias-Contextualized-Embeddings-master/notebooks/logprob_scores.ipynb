{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logprob_scores.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-irani/Quantifying-Bias-Contextualized-Embeddings/blob/master/notebooks/logprob_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX1oWAojD0Ci",
        "colab_type": "code",
        "outputId": "9fdfcbde-e848-4555-8061-84b9124e319b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM, BertModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import Iterable, Dict, List, TypeVar\n",
        "import random\n",
        "\n",
        "BERT_MODELS=\"./bert_models/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.1->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G07TU2FqIUow",
        "colab_type": "code",
        "outputId": "ce6a0f8e-184c-4189-ddad-b9c483594091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp\n",
        "from allennlp.common.util import get_spacy_model\n",
        "from spacy.attrs import ORTH\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
        "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data import Vocabulary\n",
        "\n",
        "T = TypeVar('T')\n",
        "nlp = get_spacy_model(\"en_core_web_sm\", pos_tags=False, parse=True, ner=False)\n",
        "nlp.tokenizer.add_special_case(\"[MASK]\", [{ORTH: \"[MASK]\"}])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.15.0)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.14)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.7)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (20.5.0)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.13.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.38.0)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.6/dist-packages (from flask-cors>=3.0.7->allennlp) (1.12.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.86)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.1.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.16.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIQ9IESfD43E",
        "colab_type": "code",
        "outputId": "b9460697-0df5-4411-c61b-6d81f5ea6002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI4hIGdzxLo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Final Projects/Deep Learning/Equity-Evaluation-Corpus.csv')\n",
        "np.unique(df[df['Emotion word'].notna()]['Emotion word'])\n",
        "\n",
        "names = df.loc[df.Emotion.isin(['fear', 'joy']), ['Person', 'Race']]\n",
        "european_names = df.loc[df.Race == 'European', 'Person'].drop_duplicates()\n",
        "african_american_names = df.loc[df.Race == 'African-American', 'Person'].drop_duplicates()\n",
        "\n",
        "emotions = df.loc[df.Emotion.isin(['fear', 'joy']), :]\n",
        "emotions = df.loc[df.Template.str.contains('<emotion word>'), 'Emotion word'].drop_duplicates()\n",
        "templates = df.loc[df.Emotion.isin(['fear', 'joy']), 'Template'].drop_duplicates()\n",
        "templates = [template for template in templates if '<person subject>' in template and '<emotion word>' in template]\n",
        "templates = [template.replace('<person subject>', '[MASK]') for template in templates]\n",
        "\n",
        "df = df.loc[df.Emotion.isin(['fear', 'joy']), :].drop_duplicates()\n",
        "df = df.loc[df.Person.isin(names.Person), :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xauee6hsxP6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "white_names = pd.read_csv('/content/drive/My Drive/Final Projects/Deep Learning/White-Male-Names.csv').rename(columns={' first name': 'first_name'})\n",
        "white_names = list(np.unique(list(white_names.apply(lambda x: x.first_name.split()[0], axis=1))))\n",
        "random.shuffle(white_names)\n",
        "black_names = pd.read_csv('/content/drive/My Drive/Final Projects/Deep Learning/Black-Male-Names.csv').rename(columns={'first name': 'first_name'})\n",
        "black_names = list(np.unique(list(black_names.apply(lambda x: x.first_name.split()[0], axis=1))))\n",
        "random.shuffle(black_names)\n",
        "names = white_names + black_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkL5zabSHSUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "class BertPreprocessor:\n",
        "    def __init__(self, model_type: str, max_seq_len: int=128):\n",
        "        self.model_type = model_type\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.token_indexer = PretrainedBertIndexer(\n",
        "            pretrained_model=self.model_type,\n",
        "            max_pieces=self.max_seq_len,\n",
        "            do_lowercase=True,\n",
        "        )\n",
        "        self.vocab = Vocabulary()\n",
        "        self.token_indexer._add_encoding_to_vocabulary(self.vocab)\n",
        "        self.full_vocab = {v:k for k, v in self.token_indexer.vocab.items()}\n",
        "        # self.add_vocab(names)\n",
        "\n",
        "    def tokenize(self, x: str) -> List[Token]:\n",
        "        return [Token(w) for w in flatten([\n",
        "                self.token_indexer.wordpiece_tokenizer(w)\n",
        "                for w in spacy_tok(x)]\n",
        "        )[:self.max_seq_len]]\n",
        "\n",
        "    def index_to_token(self, idx: int) -> str:\n",
        "        return self.full_vocab[idx]\n",
        "\n",
        "    def indices_to_tokens(self, indices: Iterable[int]) -> List[str]:\n",
        "        return [self.index_to_word(x) for x in indices]\n",
        "\n",
        "    def token_to_index(self, token: str,\n",
        "                      accept_wordpiece: bool=False,\n",
        "                      ) -> int:\n",
        "        wordpieces = self.tokenize(token)\n",
        "        # print(wordpieces)\n",
        "        if len(wordpieces) > 1 and not accept_wordpiece:\n",
        "            raise TokenizationError(f\"{token} is not a single wordpiece\")\n",
        "        else: token = wordpieces[0].text\n",
        "        return self.token_indexer.vocab[token]\n",
        "\n",
        "    def get_index(self, sentence: str,\n",
        "                  word: str,\n",
        "                  accept_wordpiece: bool=False,\n",
        "                  last: bool=False) -> int:\n",
        "        toks = self.tokenize(sentence)\n",
        "        wordpieces = self.tokenize(word)\n",
        "        if len(wordpieces) > 1 and not accept_wordpiece:\n",
        "            raise TokenizationError(f\"{word} is not a single wordpiece\")\n",
        "        else: word = wordpieces[0].text # use first wordpiece\n",
        "\n",
        "        if not last:\n",
        "            for i, t in enumerate(toks):\n",
        "                if t.text == word:\n",
        "                    return i + 1 # take the [CLS] token into account\n",
        "        else:\n",
        "            for i, t in enumerate(reversed(toks)):\n",
        "                if t.text == word:\n",
        "                    return len(toks) - 1 - i\n",
        "        raise ValueError(f\"No {word} tokenn tokens {toks} found\")\n",
        "\n",
        "    def to_bert_model_input(self, input_sentence: str) -> np.ndarray:\n",
        "        input_toks = self.tokenize(input_sentence)\n",
        "        batch = self.token_indexer.tokens_to_indices(input_toks, self.vocab, \"tokens\")\n",
        "        token_ids = torch.LongTensor(batch[\"tokens\"]).unsqueeze(0)\n",
        "        return token_ids\n",
        "\n",
        "    def add_vocab(self, names: List) -> None:\n",
        "        for i, name in enumerate(names):\n",
        "            self.full_vocab[i] = name\n",
        "\n",
        "def flatten(x: List[List[T]]) -> List[T]:\n",
        "    return [item for sublist in x for item in sublist]\n",
        "\n",
        "def spacy_tok(s: str) -> List[str]:\n",
        "    return [w.text for w in nlp(s)]\n",
        "\n",
        "config = Config(\n",
        "    model_type=\"bert-base-uncased\",\n",
        "    max_seq_len=128,\n",
        ")\n",
        "\n",
        "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53tZCsQJmuz",
        "colab_type": "code",
        "outputId": "c5e22c65-1b1f-461d-c778-5352291dfed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw__Y0Y5EECU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr9nqHo9ELJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd4PAxrej4vN",
        "colab_type": "code",
        "outputId": "3ee32160-97e8-4291-b50c-a371fa02ae3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "black_names[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lebron'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvhajcjZENjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_logits(sentence: str) -> np.ndarray:\n",
        "    return model(processor.to_bert_model_input(sentence))[0, :, :].cpu().detach().numpy()\n",
        "\n",
        "def softmax(arr, axis=1):\n",
        "    e = np.exp(arr)\n",
        "    return e / e.sum(axis=axis, keepdims=True)\n",
        "\n",
        "def get_mask_fill_logits(sentence: str, words: Iterable[str],\n",
        "                         use_last_mask=False, apply_softmax=False) -> Dict[str, float]:\n",
        "    mask_i = processor.get_index(sentence, \"[MASK]\", last=use_last_mask)\n",
        "    logits = defaultdict(list)\n",
        "    out_logits = get_logits(sentence)\n",
        "    if apply_softmax: \n",
        "        out_logits = softmax(out_logits)\n",
        "    return {w: out_logits[mask_i, processor.token_to_index(w)] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhpnMeuFFQLw",
        "colab_type": "text"
      },
      "source": [
        "Templates (step 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOglRQq8W-XB",
        "colab_type": "code",
        "outputId": "487eb27b-f05b-460f-8d28-382694aa313a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(list(emotions)[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM2sjPfXE-oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "templates = ['[TARGET] is [ATTRIBUTE]']\n",
        "attributes = black_names#[:20]#[10:20] #['Jamal', 'Harry']\n",
        "# white_names = ['Molly', 'Amy', 'Claire', 'Jake', 'Connor', 'Tanner']\n",
        "# black_names = ['Aaliyah', 'Precious', 'Nia', 'Terrell', 'Malik', 'Trevon']\n",
        "emotions = list(emotions)\n",
        "targets = [('happy', 'sad'), ('smart', 'dumb'), ('rich', 'poor'), ('innocent', 'guilty'), ('harmless', 'dangerous')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twWfCIRrFVH6",
        "colab_type": "text"
      },
      "source": [
        "Compute $P_{target}$=P([MASK]=[TARGET]|template) (step 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgDRsYFXJ4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGYgx0oSFUGz",
        "colab_type": "code",
        "outputId": "f187a4f8-58c8-406b-8082-250ae2e88624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "template = templates[0]\n",
        "black_name, white_name = targets\n",
        "# attribute = \"Harry\"\n",
        "\n",
        "result =  {\n",
        "    \"race_fill_bias\": 0, \"race_fill_prior_correction\": 0, \"race_fill_bias_prior_corrected\": 0, \"target_fill_bias\": 0, \n",
        "    }\n",
        "worked = []\n",
        "for attribute in attributes:\n",
        "    attribute = str(attribute)\n",
        "    if len(worked) == 100: break\n",
        "    # print(attribute)\n",
        "    # prob of [MASK] being a name given attribute\n",
        "    try:\n",
        "        subject_fill_logits = get_mask_fill_logits(\n",
        "            template.replace('[ATTRIBUTE]', attribute).replace('[TARGET]', '[MASK]'),\n",
        "            targets, False\n",
        "        )\n",
        "\n",
        "        subject_fill_bias = subject_fill_logits[white_name] - subject_fill_logits[black_name]\n",
        "        subject_fill_prior_logits = get_mask_fill_logits(\n",
        "                template.replace(\"[TARGET]\", \"[MASK]\").replace(\"[ATTRIBUTE]\", \"[MASK]\"), \n",
        "                targets, use_last_mask=True,\n",
        "        )\n",
        "\n",
        "        subject_fill_bias_prior_correction = subject_fill_prior_logits[white_name] - subject_fill_prior_logits[black_name]\n",
        "\n",
        "        # import pdb; pdb.set_trace()\n",
        "        white_name_fill_prob = get_mask_fill_logits(\n",
        "                    template.replace(\"[TARGET]\", white_name).replace(\"[ATTRIBUTE]\", \"[MASK]\"), [attribute],\n",
        "                    apply_softmax=True,\n",
        "        )[attribute]\n",
        "\n",
        "        black_name_fill_prob = get_mask_fill_logits(\n",
        "                    template.replace(\"[TARGET]\", black_name).replace(\"[ATTRIBUTE]\", \"[MASK]\"), [attribute],\n",
        "                    apply_softmax=True,\n",
        "        )[attribute]\n",
        "        # print(attribute)\n",
        "\n",
        "        tgt_fill_bias = np.log(white_name_fill_prob / black_name_fill_prob)\n",
        "        worked.append(attribute)\n",
        "        result[\"race_fill_bias\"] += subject_fill_bias\n",
        "        result[\"race_fill_prior_correction\"] += subject_fill_bias_prior_correction\n",
        "        result[\"race_fill_bias_prior_corrected\"] += subject_fill_bias - subject_fill_bias_prior_correction\n",
        "        result[\"target_fill_bias\"] += tgt_fill_bias\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # {\"race_fill_bias\": subject_fill_bias,\n",
        "    # \"race_fill_prior_correction\": subject_fill_bias_prior_correction,\n",
        "    # \"race_fill_bias_prior_corrected\": subject_fill_bias - subject_fill_bias_prior_correction,\n",
        "    # \"target_fill_bias\": tgt_fill_bias, \n",
        "    # }\n",
        "result[\"race_fill_bias\"] /= len(worked)\n",
        "result[\"race_fill_prior_correction\"] /= len(worked)\n",
        "result[\"race_fill_bias_prior_corrected\"] /= len(worked)\n",
        "result[\"target_fill_bias\"] /= len(worked)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'race_fill_bias': -1.5513824635744096,\n",
              " 'race_fill_bias_prior_corrected': 1.9053316915035248,\n",
              " 'race_fill_prior_correction': -3.456714153289795,\n",
              " 'target_fill_bias': -0.7356424981355667}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muETG_LqROt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHwoFlENG3xE",
        "colab_type": "code",
        "outputId": "74b81ff6-8c0e-4897-e5e7-f3512a2da7d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'race_fill_bias': -1.5527205592393876,\n",
              " 'race_fill_bias_prior_corrected': 1.9039935839176179,\n",
              " 'race_fill_prior_correction': -3.456714153289795,\n",
              " 'target_fill_bias': -0.7829790530353784}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VlRk3sbnrSR",
        "colab_type": "code",
        "outputId": "7d01cb25-2b54-4768-f68e-46f0e7cb058c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tgt_fill_bias"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.4316108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JeyFumInt0-",
        "colab_type": "code",
        "outputId": "5a7f332d-529b-4f5c-a698-8a7119b99b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(worked)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PSoxPAeFRQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Targets\n",
        "e_names = ['Molly', 'Amy', 'Claire', 'Jake', 'Connor', 'Tanner']\n",
        "aa_names = ['Aaliyah', 'Precious', 'Nia', 'Terrell', 'Malik', 'Trevon']\n",
        "\n",
        "# Attributes\n",
        "attributes = [['happy', 'sad'], ['smart', 'dumb'], ['rich', 'poor'], ['innocent', 'guilty'], ['harmless', 'dangerous']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUmtA6azZ4So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_target_bias(template: str, attributes: List[str], target: str) -> Dict[str, float]:\n",
        "    # P tgt\n",
        "    subject_fill_logits = get_mask_fill_logits(\n",
        "        template.replace('[TARGET]', '[MASK]').replace('[ATTRIBUTE]', target), attributes, True\n",
        "    )\n",
        "    \n",
        "    subject_fill_bias = subject_fill_logits[attributes[0]] - subject_fill_logits[attributes[1]]\n",
        "    # P prior\n",
        "    subject_fill_prior_logits = get_mask_fill_logits(\n",
        "            template.replace(\"[TARGET]\", \"[MASK]\").replace(\"[ATTRIBUTE]\", \"[MASK]\"), attributes, use_last_mask=True,\n",
        "    )\n",
        "    subject_fill_bias_prior_correction = subject_fill_prior_logits[attributes[0]] - subject_fill_prior_logits[attributes[1]]\n",
        "\n",
        "    # return {k:subject_fill_logits[k] - subject_fill_prior_logits[k] for k in attributes}\n",
        "   \n",
        "   # P prior\n",
        "    attribute1_fill_prob = get_mask_fill_logits(\n",
        "            template.replace(\"[TARGET]\", attributes[0]).replace(\"[ATTRIBUTE]\", \"[MASK]\"), [target], apply_softmax=True,\n",
        "    )[target]\n",
        "    attribute2_fill_prob = get_mask_fill_logits(\n",
        "            template.replace(\"[TARGET]\", attributes[1]).replace(\"[ATTRIBUTE]\", \"[MASK]\"), [target], apply_softmax=True,\n",
        "    )[target]\n",
        "\n",
        "    tgt_fill_bias = np.log(attribute1_fill_prob / attribute2_fill_prob)\n",
        "    return tgt_fill_bias\n",
        "    return {\"race_fill_bias\": subject_fill_bias,\n",
        "            \"race_fill_prior_correction\": subject_fill_bias_prior_correction,\n",
        "            \"race_fill_bias_prior_corrected\": subject_fill_bias - subject_fill_bias_prior_correction,\n",
        "            \"target_fill_bias\": tgt_fill_bias, \n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVsG50QGXcZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for attribute in attributes:\n",
        "    e_biases = [compute_target_bias('[TARGET] is [ATTRIBUTE]', attribute, name) for name in e_names]\n",
        "    aa_biases = [compute_target_bias('[TARGET] is [ATTRIBUTE]', attribute, name) for name in aa_names]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbrcus5uF78O",
        "colab_type": "code",
        "outputId": "4e23965e-ad22-4904-d8c6-66543fb892b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "attributes = [['guilty', 'innocent'], ['violent', 'calm'], ['rich', 'poor'],\n",
        "             ['tall', 'short'], ['clean', 'dirty'], ['leader', 'follower']]\n",
        "\n",
        "for attribute in attributes:\n",
        "    e_biases = {a:compute_target_bias('[TARGET] man is [ATTRIBUTE]', ['black', 'white'], a) for a in attribute}\n",
        "    print(e_biases)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'guilty': -0.054329403, 'innocent': -0.3128583}\n",
            "{'violent': 0.22355364, 'calm': -0.08398721}\n",
            "{'rich': -0.9035441, 'poor': -0.7467277}\n",
            "{'tall': -0.21733312, 'short': -0.4530817}\n",
            "{'clean': -0.6003376, 'dirty': -0.09724022}\n",
            "{'leader': -0.22498813, 'follower': -0.07254454}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNxZOhS2HuTe",
        "colab_type": "code",
        "outputId": "0a89442d-24ed-4a50-85fd-3ffbbee1382f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "compute_target_bias('The [TARGET] man is [ATTRIBUTE]', ['black', 'white'], 'violent')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'race_fill_bias': 1.49279,\n",
              " 'race_fill_bias_prior_corrected': 1.3480259,\n",
              " 'race_fill_prior_correction': 0.14476407,\n",
              " 'target_fill_bias': 0.6679454}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1Tp57UHxxR",
        "colab_type": "code",
        "outputId": "a23654c0-1a46-4f38-c380-1c3310f0c12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e_biases"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.415718, -1.415718, -1.415718, -1.415718, -1.415718, -1.415718]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeorF3qGJnL2",
        "colab_type": "code",
        "outputId": "8aa0b41f-3f5a-4d1f-b98d-a21cc2d2890c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "aa_biases"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1.415718, -1.415718, -1.415718, -1.415718, -1.415718, -1.415718]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe454xE-MBA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}